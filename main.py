# main.py
import torch
from torch.nn import CrossEntropyLoss

from model.custom_t5 import CustomT5Model
from dataset.data_processor import DataProcessor
# ä½¿ç”¨ helper/utils.py çš„ collate_fn
from helper.utils import collate_fn
from helper.trainer import Trainer
import argparse
import logging
import os
import json
import shutil
from helper.inference_efficiency import measure_inference_time, get_vram_usage
from transformers import set_seed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser(description="Train a custom T5 model with LoRA and MoE for Continual Learning.")
    parser.add_argument('--data_file', type=str, required=True, help='Path to the training data file (JSON).')
    parser.add_argument('--labels_file', type=str, required=True, help='Path to the labels file (JSON).')
    parser.add_argument('--model_path', type=str, default=None, help='Path to the pre-trained model to load (optional).')
    parser.add_argument('--output_dir', type=str, required=True, help='Directory to save the fine-tuned model.')
    parser.add_argument('--eval_file', type=str, required=True, help='Path to the evaluation data file (JSON).')
    parser.add_argument('--eval_labels_files', type=str, required=True, help='Path to the labels file (JSON).')
    parser.add_argument('--test_data_files', type=str, nargs='*', default=[], help='List of test data files (JSON).')
    parser.add_argument('--test_labels_files', type=str, nargs='*', default=[], help='List of test labels files (JSON).')
    parser.add_argument('--seed', type=int, help='seed')
    return parser.parse_args()

def save_custom_model(custom_model, output_dir):
    """
    è‡ªè¨‚ä¿å­˜æµç¨‹ï¼šä¿å­˜ state_dict å’Œè‡ªå®šç¾©é…ç½®ã€‚
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹çš„ state_dict
    state_dict_path = os.path.join(output_dir, "model_state_dict.pt")
    torch.save(custom_model.model.state_dict(), state_dict_path)
    logger.info(f"Model state_dict saved to {state_dict_path}")
    
    # ä¿å­˜è‡ªå®šç¾©é…ç½®
    config = {
        "num_experts": custom_model.num_experts,
        "expert_rank": custom_model.expert_rank
    }
    config_path = os.path.join(output_dir, "custom_config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=4)
    logger.info(f"Custom config saved to {config_path}")

def load_custom_model(load_directory, device):
    """
    è‡ªè¨‚åŠ è¼‰æµç¨‹ï¼šåŠ è¼‰ state_dict å’Œè‡ªå®šç¾©é…ç½®ï¼Œä¸¦åˆå§‹åŒ–æ¨¡å‹ã€‚
    """
    custom_model = CustomT5Model.load_pretrained(load_directory, device=device)
    logger.info(f"Model loaded from {load_directory}")
    return custom_model

if __name__ == "__main__":
    args = parse_args()
    
    # è¨­å®šéš¨æ©Ÿç¨®å­
    set_seed(args.seed)

    data_file = args.data_file
    labels_file = args.labels_file
    model_path = args.model_path
    output_dir = args.output_dir
    eval_file = args.eval_file
    eval_labels_files = args.eval_labels_files
    test_data_files = args.test_data_files
    test_labels_files = args.test_labels_files
    
    # ç›´æ¥åˆªé™¤èˆŠçš„ `output_dir` ä¸¦é‡æ–°å»ºç«‹
    if os.path.exists(output_dir):
        print(f"ğŸ—‘ï¸ åˆªé™¤èˆŠçš„è¼¸å‡ºç›®éŒ„: {output_dir}")
        shutil.rmtree(output_dir)  # **åˆªé™¤æ•´å€‹ç›®éŒ„**
    os.makedirs(output_dir, exist_ok=True)  # **é‡æ–°å»ºç«‹æ–°çš„ç©ºç›®éŒ„**
    print(f"âœ… å·²é‡æ–°å»ºç«‹è¼¸å‡ºç›®éŒ„: {output_dir}")

    base_model_path = "./initial_model/t5-large"
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # batch_size = 4
    # max_input_length = 256
    # max_label_length = 16

    # OLoRA çš„é…ç½® ä½†max_input_length=512æœƒOOM
    batch_size = 4
    max_input_length = 256
    max_label_length = 50

    # è®€å–è¨“ç·´è³‡æ–™
    train_processor = DataProcessor(
        data_file=data_file,
        labels_file=labels_file,
        peft_model_path=base_model_path,
        max_input_length=max_input_length,
        max_label_length=max_label_length
    )
    train_dataset = train_processor.get_dataset()
    train_dataloader = train_processor.get_dataloader(
        train_dataset, 
        batch_size=batch_size, 
        collate_fn=collate_fn  # utils.py çš„ collate_fn
    )
    #DEL å‰µå»ºè¨“ç·´é›†çš„ 500 ç­†å­é›†
    train_subset_dataloader = train_processor.get_subset_dataloader(
        train_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        subset_size=500,
        shuffle=True
    )

    # é©—è­‰è³‡æ–™
    eval_processor = DataProcessor(
        data_file=eval_file,
        labels_file=eval_labels_files,
        peft_model_path=base_model_path,
        max_input_length=max_input_length,
        max_label_length=max_label_length
    )
    eval_dataset = eval_processor.get_dataset()
    eval_dataloader = eval_processor.get_dataloader(
        eval_dataset, 
        batch_size=batch_size, 
        collate_fn=collate_fn,
    )
    #DEL å‰µå»ºé©—è­‰é›†çš„ 500 ç­†å­é›†
    eval_subset_dataloader = eval_processor.get_subset_dataloader(
        eval_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        subset_size=500,
        shuffle=True
    )

    # å»ºç«‹ T5 + LoRA æ¨¡å‹
    # å¦‚æœæœ‰æŒ‡å®šæ¨¡å‹è·¯å¾‘ï¼Œå‰‡åŠ è¼‰æ¨¡å‹ï¼Œå¦å‰‡åˆå§‹åŒ–æ–°æ¨¡å‹
    if model_path:
        logger.info(f"Loading model from {model_path}...")
        if not os.path.exists(model_path):
            logger.error(f"æŒ‡å®šçš„æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
            raise FileNotFoundError(f"æŒ‡å®šçš„æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
        # custom_model = CustomT5Model.load_pretrained(model_path, device=device)
        custom_model = load_custom_model(model_path, device=device)
        logger.info("Model loaded successfully.")
    else:
        logger.info("Initializing new model...")
        custom_model = CustomT5Model(base_model_path, device=device, num_experts=4, expert_rank=8) # è¨­ç½®å°ˆå®¶æ•¸é‡å’Œæ¯å€‹å°ˆå®¶çš„ç§©
        logger.info("Model initialized successfully.")

    model = custom_model.model  # é€™é‚Šä½¿ç”¨çš„æ˜¯ T5ForConditionalGeneration

    # å‡çµé™¤äº† LoRA/MoE ä»¥å¤–çš„åƒæ•¸
    # å‡çµæ‰€æœ‰æ¨¡å‹åƒæ•¸ then è§£å‡LoRAç›¸é—œåƒæ•¸å’Œå°ˆå®¶å±¤çš„åƒæ•¸
    logger.info("å‡çµé™¤äº† LoRA/MoE ä»¥å¤–çš„åƒæ•¸...")
    for param in model.parameters():
        param.requires_grad = False
    for name, param in model.named_parameters():
        if "lora_A" in name or "lora_B" in name:
            param.requires_grad = True
        if "experts" in name:
            param.requires_grad = True
            # print(name)
    logger.info("Parameter freezing completed.")
    
    # æª¢ç´¢æ¨¡å‹çµæ§‹
    # print(model)
    # for name, param in model.named_parameters():
    #     if "router" in name or "experts" in name or "lora_A" in name or "lora_B" in name:
    #         print(name)
    # åˆå§‹åŒ– Trainer
    trainer = Trainer(
        model=model,
        train_dataloader=train_dataloader,
        eval_dataloader=eval_dataloader,
        # DEL
        # train_dataloader=train_subset_dataloader,
        # eval_dataloader=eval_subset_dataloader,
        tokenizer=train_processor.tokenizer,
        labels_list=train_processor.labels_list,
        device=device
    )
    
    allocated_before, total, usage_before = get_vram_usage()

    # é–‹å§‹è¨“ç·´
    # trainer.train(
    #     num_epochs=1,
    #     learning_rate=5e-6,       
    #     output_dir=output_dir,
    #     accumulation_steps=2
    # )

    # # OLoRA çš„é…ç½® 
    trainer.train(
        num_epochs=3,
        learning_rate=5e-4,       
        output_dir=output_dir,
        accumulation_steps=64
    )

    # allocated_after, _, usage_after = get_vram_usage()

    # print(f"æ¨¡å‹æœ¬èº« GPU VRAM ä½¿ç”¨é‡: {allocated_before} MiB / {total} MiB ({usage_before:.2%})")
    # print(f"æ¨¡å‹è¨“ç·´ GPU VRAM ä½¿ç”¨é‡: {allocated_after} MiB / {total} MiB ({usage_after:.2%})")

    # ä¿å­˜æ¨¡å‹
    logger.info(f"Saving model to {output_dir}...")
    # custom_model.save_pretrained(output_dir)
    save_custom_model(custom_model, output_dir)
    logger.info("Model saved successfully.")



    # æ¸¬è©¦è³‡æ–™
    if test_data_files and test_labels_files:
        if len(test_data_files) != len(test_labels_files):
            logger.error("æ¸¬è©¦æ•¸æ“šæ–‡ä»¶å’Œæ¨™ç±¤æ–‡ä»¶çš„æ•¸é‡ä¸åŒ¹é…ã€‚")
            raise ValueError("æ¸¬è©¦æ•¸æ“šæ–‡ä»¶å’Œæ¨™ç±¤æ–‡ä»¶çš„æ•¸é‡ä¸åŒ¹é…ã€‚")
        
        logger.info("Starting testing on provided datasets...")
        for test_data, test_labels in zip(test_data_files, test_labels_files):
            logger.info(f"Testing on dataset: {test_data}")
            test_processor = DataProcessor(
                data_file=test_data,
                labels_file=test_labels,
                peft_model_path=base_model_path, 
                max_input_length=max_input_length,
                max_label_length=max_label_length
            )
            test_dataset = test_processor.get_dataset()
            test_dataloader = test_processor.get_dataloader(
                test_dataset, 
                batch_size=batch_size, 
                collate_fn=collate_fn,
            )
            # DEL å‰µå»ºæ¸¬è©¦é›†çš„ 500 ç­†å­é›†
            test_subset_dataloader = test_processor.get_subset_dataloader(
                test_dataset,
                batch_size=batch_size,
                collate_fn=collate_fn,
                subset_size=500,
                shuffle=True
            )
            trainer.eval_dataloader = test_dataloader
            # DEL
            # trainer.eval_dataloader = test_subset_dataloader
            test_accuracy = trainer.validate()
            logger.info(f"Test Accuracy on {test_data}: {test_accuracy:.4f}")
    else:
        logger.info("No test datasets provided. Skipping testing.")

    # æ¸¬è©¦æ¨ç†æ™‚é–“
    # measure_inference_time(model, test_dataloader, device)